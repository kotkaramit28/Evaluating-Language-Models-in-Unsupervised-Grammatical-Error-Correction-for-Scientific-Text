{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yooeR1tKtM4c"
      },
      "source": [
        "### Install LM-Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR4hMieQy_Ce"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0\n",
        "!pip install transformers==4.3.3 datasets==1.3.0 absl-py rouge-score\n",
        "!pip install nltk wandb editdistance spacy==3.0.5\n",
        "!python3 -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHusUqW30dfD",
        "outputId": "a463e3d4-e658-4305-fc31-cff9570d5e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_DinObm2tNk"
      },
      "source": [
        "!unzip /content/drive/MyDrive/LM-Critic-main.zip -d /content/drive/MyDrive/Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SI7XKL01tGD",
        "outputId": "1ba26355-1cc1-4b26-88c0-1bcfe4b18507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Project/LM-Critic-main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Project/LM-Critic-main/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vSdEFzaIm4O",
        "outputId": "f0616d28-051b-4573-f4a9-1eafe397c108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Project/LM-Critic-main/gec\n"
          ]
        }
      ],
      "source": [
        "%cd gec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3heN31k41Hu",
        "outputId": "5bc307b4-8fea-4cbd-bad7-7f0b7cbad5f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = open('data/round0_synthetic_scigpt/synthetic_50k.json','r')\n",
        "d = [i for i in f]\n",
        "f.close()\n",
        "f = open('data/round0_synthetic_scigpt_10k/synthetic_10k.json','w')\n",
        "[f.write(i) for i in d[:10000]]\n",
        "f.close()\n",
        "len(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsVkdOvwgOl"
      },
      "source": [
        "Test Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VURZVyA306m-",
        "outputId": "037ad459-07a9-41e3-caa0-2d3738557ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CKP 0\n",
            "CKP 1\n",
            "Loaded ../scigpt/best-checkpoint\n",
            "CKP 2\n",
            "Enter a sentence: it is also expected to improve the process by managing the factors , ultimately achieving an improvement in construction productivity\n",
            "Bad! Your sentence log(p) = -92.737\n",
            "Neighbor sentence with highest log(p): , it is also expected to improve the process by managing the factors, ultimately achieving an improvement in construction productivity (= -91.835)\n",
            "Enter a sentence: it is also expected to improve the process by managing the factors, ultimately achieving an improvement in construction productivity\n",
            "Good! Your sentence log(p) = -92.737\n",
            "Enter a sentence: Traceback (most recent call last):\n",
            "  File \"../critic/critic_scigpt.py\", line 140, in <module>\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python3 ../critic/critic_scigpt.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T-2x1aTATaH"
      },
      "source": [
        "Eval Critic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('eval.json','r')\n",
        "d = [i for i in f]\n",
        "f.close()\n",
        "len(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO3gwcaHQnvM",
        "outputId": "a0dfdd51-150f-4e3d-f741-833527a0fec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1949"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('eval_ledat.json','w')\n",
        "[f.write(i) for i in d[:600]]\n",
        "f.close()"
      ],
      "metadata": {
        "id": "l22TCdBYQv9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../gec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2viS1nUQRAKb",
        "outputId": "fed5092b-4551-4fd7-e19b-4e5e2acd6b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Project/LM-Critic-main/gec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtKQYHZOAXUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1424b4-da7b-4f62-c822-e620966ecb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CKP 0\n",
            "CKP 1\n",
            "Loaded ../scigpt/best-checkpoint\n",
            "CKP 2\n",
            "100% 20/20 [00:03<00:00,  6.51it/s]\n",
            "100% 20/20 [00:02<00:00,  6.86it/s]\n",
            "log p(bad) < log p(good)? 387 / 600 = 0.645\n",
            "\n",
            "Baseline critic:\n",
            "  Good precision = 344 / 683 = 0.504\n",
            "  Good recall    = 344 / 600 = 0.573\n",
            "  Good F0.5      = 0.516\n",
            "  Bad precision  = 261 / 517 = 0.505\n",
            "  Bad recall     = 261 / 600 = 0.435\n",
            "  Bad F0.5       = 0.489\n",
            "NO_SAMPLES 100\n",
            "100% 600/600 [02:18<00:00,  4.34it/s]\n",
            "100% 600/600 [02:14<00:00,  4.47it/s]\n",
            "\n",
            "LM-Critic:\n",
            "  Good precision = 345 / 660 = 0.523\n",
            "  Good recall    = 345 / 600 = 0.575\n",
            "  Good F0.5      = 0.532\n",
            "  Bad precision  = 285 / 540 = 0.528\n",
            "  Bad recall     = 285 / 600 = 0.475\n",
            "  Bad F0.5       = 0.516\n"
          ]
        }
      ],
      "source": [
        "!python ../eval_critic/eval_critic.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZvsTabutcAe"
      },
      "source": [
        "### Train Initial Fixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsmf30jkyuh8"
      },
      "outputs": [],
      "source": [
        "path = 'data/round2_scigpt'\n",
        "gpt = 'scigpt'\n",
        "gpt_cmd = '-scigpt True' # -scigpt True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH5_cpcJy7ND"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {path}/fixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO701S48pHD5",
        "outputId": "5712127e-f9b9-4964-906d-a1f85848a244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/22/2022 13:08:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/22/2022 13:08:47 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='data/round2_scigpt/fixer', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Jul22_13-08-47_e02edc818058', logging_first_step=True, logging_steps=20, save_steps=2000, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='data/round2_scigpt/fixer', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, sortish_sampler=False, predict_with_generate=True)\n",
            "07/22/2022 13:08:47 - WARNING - datasets.builder -   Using custom data configuration default-45202a27ac895240\n",
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-45202a27ac895240/0.0.0/5068e8663a7669137d288ea22cd76c5f4cac5f20db7ed8252b722e51c43c0760...\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-45202a27ac895240/0.0.0/5068e8663a7669137d288ea22cd76c5f4cac5f20db7ed8252b722e51c43c0760. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/facebook/bart-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm0ml5v_9\n",
            "Downloading: 100%|██████████| 1.72k/1.72k [00:00<00:00, 1.48MB/s]\n",
            "storing https://huggingface.co/facebook/bart-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "creating metadata file for /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "https://huggingface.co/roberta-large/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfdq9tl7h\n",
            "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 7.69MB/s]\n",
            "storing https://huggingface.co/roberta-large/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "creating metadata file for /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "https://huggingface.co/roberta-large/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp81eab4xd\n",
            "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.89MB/s]\n",
            "storing https://huggingface.co/roberta-large/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "creating metadata file for /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "https://huggingface.co/roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp31flzcl3\n",
            "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 10.4MB/s]\n",
            "storing https://huggingface.co/roberta-large/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "creating metadata file for /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkkklb6bf\n",
            "Downloading: 100%|██████████| 558M/558M [00:07<00:00, 70.9MB/s]\n",
            "storing https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "creating metadata file for /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "07/22/2022 13:09:06 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121c13b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "#0:   0%|          | 0/1 [00:00<?, ?ba/s]07/22/2022 13:09:06 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cd3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "#0: 100%|██████████| 1/1 [00:00<00:00,  3.29ba/s]\n",
            "#1: 100%|██████████| 1/1 [00:00<00:00,  4.59ba/s]\n",
            "#0: 100%|██████████| 1/1 [00:00<00:00,  3.20ba/s]\n",
            "07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cd3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "#2:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cd3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "#3:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#4:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "#2: 100%|██████████| 1/1 [00:00<00:00,  2.78ba/s]\n",
            "\n",
            "\n",
            "\n",
            "07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "#3: 100%|██████████| 1/1 [00:00<00:00,  2.62ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#5:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "#4: 100%|██████████| 1/1 [00:00<00:00,  2.49ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#6:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/22/2022 13:09:07 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#7:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#5: 100%|██████████| 1/1 [00:00<00:00,  2.26ba/s]\n",
            "07/22/2022 13:09:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#8:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#6: 100%|██████████| 1/1 [00:00<00:00,  2.13ba/s]\n",
            "07/22/2022 13:09:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#9:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#7: 100%|██████████| 1/1 [00:00<00:00,  1.99ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#8: 100%|██████████| 1/1 [00:00<00:00,  2.42ba/s]\n",
            "07/22/2022 13:09:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#10:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#9: 100%|██████████| 1/1 [00:00<00:00,  2.64ba/s]\n",
            "07/22/2022 13:09:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#11:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/22/2022 13:09:08 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c121cc3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#12:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#10: 100%|██████████| 1/1 [00:00<00:00,  2.53ba/s]\n",
            "07/22/2022 13:09:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#13:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#11: 100%|██████████| 1/1 [00:00<00:00,  2.09ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#12: 100%|██████████| 1/1 [00:00<00:00,  2.24ba/s]\n",
            "07/22/2022 13:09:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#14:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#13: 100%|██████████| 1/1 [00:00<00:00,  2.73ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/22/2022 13:09:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "#13: 100%|██████████| 1/1 [00:00<00:00,  2.54ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#15:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/22/2022 13:09:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#16:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#14: 100%|██████████| 1/1 [00:00<00:00,  2.60ba/s]\n",
            "07/22/2022 13:09:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7b3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#17:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#15: 100%|██████████| 1/1 [00:00<00:00,  2.17ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#16: 100%|██████████| 1/1 [00:00<00:00,  2.45ba/s]\n",
            "07/22/2022 13:09:10 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7d3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#18:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#17: 100%|██████████| 1/1 [00:00<00:00,  3.13ba/s]\n",
            "07/22/2022 13:09:10 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f1c12e7d3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#18: 100%|██████████| 1/1 [00:00<00:00,  5.65ba/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#19: 100%|██████████| 1/1 [00:00<00:00,  7.04ba/s]\n",
            "Downloading: 5.61kB [00:00, 4.08MB/s]                   \n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_seq2seq.py\", line 537, in <module>\n",
            "    main()\n",
            "  File \"src/run_seq2seq.py\", line 449, in main\n",
            "    metric = load_metric(metric_name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 608, in load_metric\n",
            "    dataset=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 448, in prepare_module\n",
            "    f\"To be able to use this {module_type}, you need to install the following dependencies\"\n",
            "ImportError: To be able to use this metric, you need to install the following dependencies['rouge_score'] using 'pip install rouge_score' for instance'\n"
          ]
        }
      ],
      "source": [
        "!python -u src/run_seq2seq.py \\\n",
        "    --model_name_or_path facebook/bart-base --task summarization --text_column bad_detoked --summary_column good_detoked \\\n",
        "    --do_train --num_train_epochs 1 --train_file {path}/round_train.json \\\n",
        "    --preprocessing_num_workers 20 --overwrite_output_dir --output_dir {path}/fixer --predict_with_generate --fp16 \\\n",
        "    --per_device_train_batch_size 64 --gradient_accumulation_steps 8 --max_source_length 64 --max_target_length 64 \\\n",
        "    --logging_first_step --logging_steps 20 --save_steps 2000 \\\n",
        "  |& tee {path}/fixer/log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loKeojcNtgHs"
      },
      "source": [
        "Run Fixer on BEA-19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGXwb89Mt-j8"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9LeGJ9vzSwn",
        "outputId": "bbfa12db-e980-4b7e-c9ff-cf14169f3234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgyfP2jiti7J",
        "outputId": "d74d0728-0021-4736-eb87-4b6979b5b65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file data/round2_scigpt/fixer/config.json not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 424, in get_config_dict\n",
            "    use_auth_token=use_auth_token,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1093, in cached_path\n",
            "    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
            "OSError: file data/round2_scigpt/fixer/config.json not found\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_fixer.py\", line 22, in <module>\n",
            "    model = BartForConditionalGeneration.from_pretrained(args.model_path, force_bos_token_to_be_generated=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 962, in from_pretrained\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 376, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 436, in get_config_dict\n",
            "    raise EnvironmentError(msg)\n",
            "OSError: Can't load config for 'data/round2_scigpt/fixer'. Make sure that:\n",
            "\n",
            "- 'data/round2_scigpt/fixer' is a correct model identifier listed on 'https://huggingface.co/models'\n",
            "\n",
            "- or 'data/round2_scigpt/fixer' is the correct path to a directory containing a config.json file\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m {path}/fixer -i benchmarks/wi+locness_v2.1.bea19/m2/ABCN.dev.bea19.orig.txt -o {path}/predictions/bea19dev.out.txt --bea19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou-7GUVB-K21"
      },
      "source": [
        "BEA T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIyh3uRdzHdX",
        "outputId": "9c1d1857-4943-4730-f75f-cd63e28ddd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T5\n",
            "100% 110/110 [19:15<00:00, 10.51s/it]\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer_t5.py -m {path}/fixer -i benchmarks/wi+locness_v2.1.bea19/m2/ABCN.dev.bea19.orig.txt -o {path}/predictions/bea19dev.out1.txt --bea19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGxIbQdRWwXP"
      },
      "source": [
        "CoNLL-14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iH1V7eTRLdn",
        "outputId": "b0ede654-4f99-459e-fff4-834726bc01c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file data/round2_scigpt/fixer/config.json not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 424, in get_config_dict\n",
            "    use_auth_token=use_auth_token,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1093, in cached_path\n",
            "    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
            "OSError: file data/round2_scigpt/fixer/config.json not found\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_fixer.py\", line 22, in <module>\n",
            "    model = BartForConditionalGeneration.from_pretrained(args.model_path, force_bos_token_to_be_generated=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 962, in from_pretrained\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 376, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 436, in get_config_dict\n",
            "    raise EnvironmentError(msg)\n",
            "OSError: Can't load config for 'data/round2_scigpt/fixer'. Make sure that:\n",
            "\n",
            "- 'data/round2_scigpt/fixer' is a correct model identifier listed on 'https://huggingface.co/models'\n",
            "\n",
            "- or 'data/round2_scigpt/fixer' is the correct path to a directory containing a config.json file\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m {path}/fixer -i benchmarks/official-2014.combined.orig.txt -o {path}/predictions/conll.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtv8xClzZVoL"
      },
      "source": [
        "Run Fixer on Ledat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvKLxKuWZXQR",
        "outputId": "372b7b87-8e67-4913-ad9a-b9917c9960a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file data/round2_scigpt/fixer/config.json not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 424, in get_config_dict\n",
            "    use_auth_token=use_auth_token,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1093, in cached_path\n",
            "    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
            "OSError: file data/round2_scigpt/fixer/config.json not found\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/run_fixer.py\", line 22, in <module>\n",
            "    model = BartForConditionalGeneration.from_pretrained(args.model_path, force_bos_token_to_be_generated=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 962, in from_pretrained\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 376, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 436, in get_config_dict\n",
            "    raise EnvironmentError(msg)\n",
            "OSError: Can't load config for 'data/round2_scigpt/fixer'. Make sure that:\n",
            "\n",
            "- 'data/round2_scigpt/fixer' is a correct model identifier listed on 'https://huggingface.co/models'\n",
            "\n",
            "- or 'data/round2_scigpt/fixer' is the correct path to a directory containing a config.json file\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m {path}/fixer -i benchmarks/ledat.txt -o {path}/predictions/ledat.out.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHhVJoqT2TTY"
      },
      "source": [
        "Test Fixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhUHgkFB2UlA"
      },
      "outputs": [],
      "source": [
        "!echo \"This will prompt you for a sentence input, an returns the judgment (Good: grammatical, Bad: ungrammatical) along with the probability score of the input sentence.\" > test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL6gUCz51WuU",
        "outputId": "82644d98-e89c-43e1-c061-c756b1cbd00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 1/1 [00:00<00:00,  1.46it/s]\n",
            "mkdir: missing operand\n",
            "Try 'mkdir --help' for more information.\n",
            "this will prompt you for a sentence input , an returns the judgment ( good : grammatical , bad : ungrammatical ) along with the probability score of the input sentence\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m data/round0_synthetic/fixer -i test.txt -o test_out.txt\n",
        "!cat test_out.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fdM48wHN-k"
      },
      "source": [
        "Run Critic on Data ( Get Training File for Fixer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRV1GwMRcXRq",
        "outputId": "8cf7dd4d-3565-4342-990e-6305440b002f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project/LM-Critic-main/gec\n"
          ]
        }
      ],
      "source": [
        "%cd LM-Critic-main/gec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXAcnVNQHSLy",
        "outputId": "362b8ffc-f935-442b-b451-665d59875552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCIGPT\n",
            "CKP 0\n",
            "CKP 1\n",
            "Loaded ../scigpt/best-checkpoint\n",
            "CKP 2\n",
            "100% 10000/10000 [30:37<00:00,  5.44it/s]\n",
            "src/use_critic.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.save(f,np.array(res))\n",
            "Good Sentences 2976\n",
            "Bad Sentences 6992\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/use_critic.py -f data/unlabelled_10k.txt -g data/unlabelled_scigpt/unlabelled_gd_10k.txt -b data/unlabelled_scigpt/unlabelled_bd_10k.txt -skip_indexes True -scigpt True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT on S2ORC:\n",
        "Good Sentences 2960\n",
        "Bad Sentences 7008"
      ],
      "metadata": {
        "id": "LMiuCIOikh8W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIZMxK0VWWHZ"
      },
      "source": [
        "Run Fixer on unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb-2vL_8bn00",
        "outputId": "8ceca029-a178-43ea-b0e1-8f5fbce7fc49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 175/175 [17:44<00:00,  6.08s/it]\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m {path}/fixer -i data/unlabelled_{gpt}/unlabelled_bd_10k.txt -o {path}/data/fixer_output_unlabelled.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/run_fixer.py -m lm-data/data/round1__BIFI/model-fixer/ -i data/unlabelled_yahoo/unlabelled_bd_10k.txt -o lm-data/data/round1__BIFI/fixer_output_unlabelled.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A4DexH8kGJx",
        "outputId": "98781611-2754-43d0-cc3c-77ff6258b515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 149/149 [13:17<00:00,  5.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NJ7eJM0-i3p"
      },
      "source": [
        "Run Critic on Fixer Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt1mDTYw-j1S",
        "outputId": "a7fc28c3-b236-47a8-9547-2e35e0b84554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCIGPT\n",
            "CKP 0\n",
            "CKP 1\n",
            "Loaded ../scigpt/best-checkpoint\n",
            "CKP 2\n",
            "100% 6992/6992 [22:22<00:00,  5.21it/s]\n",
            "src/use_critic.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.save(f,np.array(res))\n",
            "Good Sentences 524\n",
            "Bad Sentences 6468\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/use_critic.py -f {path}/data/fixer_output_unlabelled.txt -g {path}/data/fixer_output_unlabelled_gd.txt -b {path}/data/fixer_output_unlabelled_bd.txt {gpt_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gpt: Good Sentences 486\n",
        "Bad Sentences 6522"
      ],
      "metadata": {
        "id": "MG8F67a446Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scigpt: Good Sentences 524\n",
        "Bad Sentences 6468"
      ],
      "metadata": {
        "id": "DUT4MBWlTRIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised gpt: Good Sentences 499\n",
        "Bad Sentences 6509"
      ],
      "metadata": {
        "id": "9OrVuOtt9G-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "supervised scigpt: Good Sentences 535\n",
        "Bad Sentences 6457"
      ],
      "metadata": {
        "id": "1APMbd4ePmxb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCyuvOCEF2eX"
      },
      "source": [
        "Extract Training File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWzPYjgK8nne",
        "outputId": "ed03fd0a-0352-4c14-85c8-374d80c823a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/extract_json.py -t {path}/data/fixer_output_unlabelled_gd.txt -p data/unlabelled_{gpt}/unlabelled_bd_10k.txt -o {path}/data/train_breaker.json -r {path}/data/train_fixer_A.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v97bjTi2khCj"
      },
      "source": [
        "Train Breaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Y98N5AkL0k"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {path}/breaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yph369V-kmLx",
        "outputId": "3e4e2088-d9b4-4c10-a775-485ba24af6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/21/2022 15:11:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/21/2022 15:11:30 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='data/round1_scigpt/breaker', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Jul21_15-11-30_af59b7f1b83b', logging_first_step=True, logging_steps=20, save_steps=2000, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='data/round1_scigpt/breaker', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, sortish_sampler=False, predict_with_generate=True)\n",
            "07/21/2022 15:11:30 - WARNING - datasets.builder -   Using custom data configuration default-aa307400d84f4245\n",
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-aa307400d84f4245/0.0.0/5068e8663a7669137d288ea22cd76c5f4cac5f20db7ed8252b722e51c43c0760...\n",
            "\r0 tables [00:00, ? tables/s]\r                            \rDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-aa307400d84f4245/0.0.0/5068e8663a7669137d288ea22cd76c5f4cac5f20db7ed8252b722e51c43c0760. Subsequent calls will reuse this data.\n",
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.3.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "07/21/2022 15:11:40 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510c9cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "07/21/2022 15:11:40 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cacb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "07/21/2022 15:11:40 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cbcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "#0: 100%|██████████| 1/1 [00:00<00:00, 17.15ba/s]\n",
            "\n",
            "#1: 100%|██████████| 1/1 [00:00<00:00, 17.02ba/s]\n",
            "\n",
            "\n",
            "#2: 100%|██████████| 1/1 [00:00<00:00, 12.80ba/s]\n",
            "07/21/2022 15:11:41 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cbcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "#3: 100%|██████████| 1/1 [00:00<00:00, 16.96ba/s]\n",
            "07/21/2022 15:11:41 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cccb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#4: 100%|██████████| 1/1 [00:00<00:00, 26.01ba/s]\n",
            "07/21/2022 15:11:42 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cccb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#5: 100%|██████████| 1/1 [00:00<00:00, 24.94ba/s]\n",
            "07/21/2022 15:11:42 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cdcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#7:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/21/2022 15:11:42 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cccb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#7: 100%|██████████| 1/1 [00:00<00:00, 15.37ba/s]\n",
            "#6: 100%|██████████| 1/1 [00:00<00:00, 10.97ba/s]\n",
            "07/21/2022 15:11:42 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#8: 100%|██████████| 1/1 [00:00<00:00, 20.35ba/s]\n",
            "07/21/2022 15:11:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#10: 100%|██████████| 1/1 [00:00<00:00, 12.30ba/s]\n",
            "07/21/2022 15:11:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#9: 100%|██████████| 1/1 [00:00<00:00, 15.32ba/s]\n",
            "07/21/2022 15:11:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cfcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#11: 100%|██████████| 1/1 [00:00<00:00, 19.12ba/s]\n",
            "07/21/2022 15:11:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cfcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#12: 100%|██████████| 1/1 [00:00<00:00, 26.58ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510cfcb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#13:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510d0cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#13: 100%|██████████| 1/1 [00:00<00:00, 18.00ba/s]\n",
            "#14: 100%|██████████| 1/1 [00:00<00:00, 18.93ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510d0cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#15: 100%|██████████| 1/1 [00:00<00:00, 47.06ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510d0cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#16: 100%|██████████| 1/1 [00:00<00:00, 29.42ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc8510d0cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#17: 100%|██████████| 1/1 [00:00<00:00, 33.40ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc851051cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#18: 100%|██████████| 1/1 [00:00<00:00, 57.50ba/s]\n",
            "07/21/2022 15:11:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fc851051cb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#19: 100%|██████████| 1/1 [00:00<00:00, 82.04ba/s]\n",
            "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: .\n",
            "Using amp fp16 backend\n",
            "***** Running training *****\n",
            "  Num examples = 524\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 1\n",
            "{'loss': 0.8949, 'learning_rate': 0.0, 'epoch': 0.89}\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4.9194, 'train_samples_per_second': 0.203, 'epoch': 0.89}\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.90s/it]\n",
            "Saving model checkpoint to data/round1_scigpt/breaker\n",
            "Configuration saved in data/round1_scigpt/breaker/config.json\n",
            "Model weights saved in data/round1_scigpt/breaker/pytorch_model.bin\n",
            "07/21/2022 15:11:57 - INFO - __main__ -   ***** Train results *****\n",
            "07/21/2022 15:11:57 - INFO - __main__ -     epoch = 0.89\n",
            "07/21/2022 15:11:57 - INFO - __main__ -     train_runtime = 4.9194\n",
            "07/21/2022 15:11:57 - INFO - __main__ -     train_samples_per_second = 0.203\n"
          ]
        }
      ],
      "source": [
        "!python -u src/run_seq2seq.py \\\n",
        "    --model_name_or_path facebook/bart-base --task summarization --text_column bad_detoked --summary_column good_detoked \\\n",
        "    --do_train --num_train_epochs 1 --train_file {path}/data/train_breaker.json \\\n",
        "    --preprocessing_num_workers 20 --overwrite_output_dir --output_dir {path}/breaker --predict_with_generate --fp16 \\\n",
        "    --per_device_train_batch_size 64 --gradient_accumulation_steps 8 --max_source_length 64 --max_target_length 64 \\\n",
        "    --logging_first_step --logging_steps 20 --save_steps 2000 \\\n",
        "  |& tee {path}/breaker/log.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13YJDN4RlMpE"
      },
      "source": [
        "Run Breaker on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq8RPfEXk1P3",
        "outputId": "f2fef1ba-5fa2-4709-f2af-29b04fefdf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 75/75 [07:01<00:00,  5.62s/it]\n"
          ]
        }
      ],
      "source": [
        "!python src/run_fixer.py -m {path}/breaker -i data/unlabelled_{gpt}/unlabelled_gd_10k.txt -o {path}/data/breaker_output_unlabelled.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsmOgvR9vXM3"
      },
      "source": [
        "Run Critic on Breaker Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59VY2SIlWS3",
        "outputId": "c1735e31-d77c-4c92-db04-eba63efdc635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCIGPT\n",
            "CKP 0\n",
            "CKP 1\n",
            "Loaded ../scigpt/best-checkpoint\n",
            "CKP 2\n",
            "100% 2976/2976 [08:11<00:00,  6.06it/s]\n",
            "src/use_critic.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.save(f,np.array(res))\n",
            "Good Sentences 2499\n",
            "Bad Sentences 477\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/use_critic.py -f {path}/data/breaker_output_unlabelled.txt -g {path}/data/breaker_output_unlabelled_gd.txt -b {path}/data/breaker_output_unlabelled_bd.txt {gpt_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt Good Sentences 2488\n",
        "Bad Sentences 472"
      ],
      "metadata": {
        "id": "LEaqZJAO8o0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scigpt: Good Sentences 2484\n",
        "Bad Sentences 492"
      ],
      "metadata": {
        "id": "cWWgBGQ_b6Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "supervised gpt: Good Sentences 2502\n",
        "Bad Sentences 458"
      ],
      "metadata": {
        "id": "Jm3bRyHz9NbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "supervised scigpt: Good Sentences 2509\n",
        "Bad Sentences 467"
      ],
      "metadata": {
        "id": "O5_WnUmjPsM-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aqk7KY35ZkV"
      },
      "source": [
        "Extract JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMXw3PbC5a_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e54a03f-2806-4aba-f010-f6903249be76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/extract_json.py -t {path}/data/breaker_output_unlabelled_bd.txt -p data/unlabelled_{gpt}/unlabelled_gd_10k.txt -o {path}/data/train_fixer.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpa2boWLw4PT"
      },
      "source": [
        "Merge JSONs to Make complete training file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7xgpor5PA8k"
      },
      "outputs": [],
      "source": [
        "new_path = 'data/round2_scigpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g7B6mHnxkjE"
      },
      "outputs": [],
      "source": [
        "!mkdir {new_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXoSAzDaw3Ox",
        "outputId": "c6f8f21d-938b-491d-c135-d509d25cb442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python src/merge_files.py -f1 data/round0_synthetic/synthetic_10k.json -f2 {path}/data/train_fixer_A.json -f3 {path}/data/train_fixer.json -o {new_path}/round_train.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-BTRSB81wXB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GEC.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}